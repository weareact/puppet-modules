# ------------------------------------------------------------------------------
#
# Copyright (c) 2016, WSO2 Inc. (http://www.wso2.org) All Rights Reserved.
#
# WSO2 Inc. licenses this file to you under the Apache License,
# Version 2.0 (the "License"); you may not use this file except
# in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# ------------------------------------------------------------------------------

# ------------------------------------------------------
# CARBON RELATED SPARK PROPERTIES
# ------------------------------------------------------
# Carbon specific properties when running Spark in the Carbon environment.
# Should start with the prefix "carbon."

# carbon.spark.master config has 3 states
#   1. (default) local mode - spark starts in the local mode (NOTE: carbon.spark.master.count property
#       will not be considered here)
#       ex: "carbon.spark.master local" or "carbon.spark.master local[2]"
#   2. client mode - DAS acts as a client for an external Spark cluster (NOTE: carbon.spark.master.count property
#       will not be considered here)
#       ex: "carbon.spark.master spark://<host name>:<port>"
#   3. cluster mode - DAS creates its own Spark cluster usign Carbon Clustering
#       ex: "carbon.spark.master local" AND "carbon.spark.master.count  <number of redundant masters>"

carbon.spark.master <%= @spark['master'] %>
carbon.spark.master.count  <%= @spark['master_count'] %>

#This configuration is used to limit the number of results returned from spark query execution
#To return all the results, set this to -1
carbon.spark.results.limit 1000

# this configuratoin can be used to point to a symbolic link to WSO2 DAS HOME
<%- if @clustering['enabled'] -%>
carbon.das.symbolic.link <%= @carbon_home_symlink %>
<%- else -%>
# carbon.das.symbolic.link /home/ubuntu/das/das_symlink/
<%- end -%>

# this configuration can be used with the spark fair scheduler, when fair schedule pools are used. the
#     defualt pool name for carbon is 'carbon-pool'
# carbon.scheduler.pool carbon-pool

# ------------------------------------------------------
# SPARK PROPERTIES
# ------------------------------------------------------
# Default system properties included when running spark.
# This is useful for setting default environmental settings.
# Check http://spark.apache.org/docs/latest/configuration.html#environment-variables for further information

# Application Properties
spark.app.name  CarbonAnalytics
spark.driver.cores 1
spark.driver.memory 512m
spark.executor.memory 512m

# Runtime Environment

# Spark UI
spark.ui.port 4040
spark.history.ui.port 18080

# Compression and Serialization
spark.serializer  org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer 256k
spark.kryoserializer.buffer.max 256m

# Execution Behavior

# Networking
spark.blockManager.port 12000
spark.broadcast.port  12500
# spark.driver.host <%= @spark['hostname'] %>
spark.driver.port 13000
spark.executor.port 13500
spark.fileserver.port 14000
spark.replClassServer.port 14500

# Scheduling
spark.scheduler.mode FAIR
# this property can be set to specify where hte fairscheduler.xml file is. the carbon specific
#     fairscheduler.xml is in the <DAS_HOME>/repository/conf/analytics/spark directory
# spark.scheduler.allocation.file <DAS_HOME>/repository/conf/analytics/spark/fairscheduler.xml

# Dynamic Allocation

# Security

# Encryption

# Standalone Cluster Configs
spark.deploy.recoveryMode CUSTOM
spark.deploy.recoveryMode.factory org.wso2.carbon.analytics.spark.core.deploy.AnalyticsRecoveryModeFactory

# Master
spark.master.port 7077
spark.master.rest.port 6066
spark.master.webui.port 8081

# Worker
spark.worker.cores 1
spark.worker.memory 1g
spark.worker.dir work
spark.worker.port 11000
spark.worker.webui.port 11500

# Spark Logging

# To allow event logging for spark you need to uncomment
# the line spark.eventlog.log true and set the directory in which the
# logs will be stored.

# spark.eventLog.enabled true
# spark.eventLog.dir <PATH_FOR_SPARK_EVENT_LOGS>

# akka settings
# akka.remote.netty.tcp.hostname
# akka.remote.netty.tcp.port 7077
# akka.remote.netty.tcp.bind-hostname
# akka.remote.netty.tcp.bind-port 7077
